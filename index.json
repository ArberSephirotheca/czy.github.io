[{"categories":["Distribution System"],"content":"Why Raft It provides a consensus algorithm allowing a collection of machines to work as a group that can survive the failures of some of its members. ","date":"2021-08-10","objectID":"/raft/:1:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Features Strong leader logs only flow from leader to others. Leader election Raft uses randomized timers to elect leaders. Membership changes Raft uses joint consensus approach where two different configurations overlap largely during transitions. ","date":"2021-08-10","objectID":"/raft/:2:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Replicated state machines State MachineAlt text \" State Machine The state machines process identical sequences of commands from the logs. Each servers stores a replicated log. ","date":"2021-08-10","objectID":"/raft/:3:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Raft Consensus algorithm It ensures safety under all non-Byzantine conditions, including: network delays partitions packet loss duplication reordering It decomposes consensus problem into three subproblems: Leader election - A leader must br chosen when an exisiting leader fails (Section 5.2). Log replication - the leader must accept log entries from clients and replicates them across the cluster (Section 5.3). Safety - see figure below. SafetyAlt text \" Safety ","date":"2021-08-10","objectID":"/raft/:4:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Basic Structure Each server has one of state: Candidate - used to elect a new leader. Follwer - simply respond to rquests from leaders and candidates. Leader - handles all client requests(follower will redirect request to leader). State ChangeAlt text \" State Change ","date":"2021-08-10","objectID":"/raft/:5:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"RPCs Two type of RPC: RequestVote RPCs - initiated by candidates. AppendEntries RPCs - initiated by leader to replicate log and send heartbeat. ","date":"2021-08-10","objectID":"/raft/:6:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Leader election When server start up, they begin as followers. Leader send periodic heartbeats(append with no log entries) to all followers. When a follower receive no message over a period (election timeout), it starts a election: Follower increments its term and becomes candidate. Candidate vote itself and issues requestVote RPCs. Candidate mantains its state until: It wins election. Another server becoms a leader. No winner over a period of time. When a candidate receives RPCs from other candidates with \u003c term, it rejects. When a candidate receives RPCs from other candidates with \u003e= term, it becomes follower. ","date":"2021-08-10","objectID":"/raft/:7:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"记录下做tinykv的想法 ","date":"2021-08-06","objectID":"/pingcap-tinykv/:0:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"高可用分布式储存系统必要的需求: 合理的副本数量 分配副本到不同的机器上 加入新节点后，要将其他节点上的副本迁移过来 节点下线后，要将下线节点的数据迁移 ","date":"2021-08-06","objectID":"/pingcap-tinykv/:1:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"高可用分布式储存系统可选的需求: 维持每个集群的 Leader 分布均匀 维持每个节点的储存容量均匀 维持访问热点分布均匀 管理 Balance 的速度，避免影响服务 管理节点状态，包括上线/下线节点，以及自动下线失效节点 ","date":"2021-08-06","objectID":"/pingcap-tinykv/:2:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"Project 1 Create DB first. CF is a key namespace where seperates same keys in different key namesapce. DB is based on badger. * It uses MVCC. * It runs transactions concurrently. * It has serializable snapshot isolation guarantees. * It uses LSM tree with value log to seperate keys from values. Each read/write opens a new transcation. Discard() after each transaction ","date":"2021-08-06","objectID":"/pingcap-tinykv/:3:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"Project 2 A: Initialize the new raft. Handle step to deal about state change and message received/sent. Each time a follower/candidate encounters election timeout, it becomes a candidate to start a new election, the term + 1. voter use first-come-first-serve rule, which only votes once in a single term. Use randomnized election timeout to prevent conflict that many followers become candiate at the same time. ","date":"2021-08-06","objectID":"/pingcap-tinykv/:4:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"Reference https://pingcap.com/blog-cn/tidb-internal-1/ https://pingcap.com/blog-cn/tidb-internal-3/. ","date":"2021-08-06","objectID":"/pingcap-tinykv/:5:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"Discuss GFS research paper ","date":"2021-08-04","objectID":"/gfs/:0:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"What is GFS A distributed file system for large distributed data-intensive application. ","date":"2021-08-04","objectID":"/gfs/:1:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Why GFS Big, fast Global data - visit data across many data center. Sharding - split data into serveral servers for parallelism. Automatic recovery - recover data from failure automatically. ","date":"2021-08-04","objectID":"/gfs/:2:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Big Storage is hard Performance -\u003e sharding(split data into serveral severs in order to operate in parallel) Faults -\u003e Tolerance Tolerance -\u003e replication Replication -\u003e Inconsistency Consistency -\u003e Low performance ","date":"2021-08-04","objectID":"/gfs/:3:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Structure of GFS GFS ArchitectureAlt text \" GFS Architecture C1, C2, C3 ... Cn | | | Master | | | Chunk server 1 Chunk server 2 Chunk server 3 ... Chunk server: Responsible for reading data. Usually has three copies of same chunk on other chunkservers. Master data: Responsible for writing data and controll chunkservers. Store file, chunk namespaces, mapping of files to chunks, locations of each chunks' replcias in RAM. file name -\u003e array of chunk handles hanlde -\u003e list of chunkservers version number primary(what chunkserver is primary, what are copies) lease expiration Store log(operation such as write, read), checkpoint(version number) in disk. ","date":"2021-08-04","objectID":"/gfs/:4:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Read name, offset -\u003e Master Master send chunk handle, list of servers Client read -\u003e Chunkserver Chunkserver return data -\u003e client ","date":"2021-08-04","objectID":"/gfs/:5:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Write Write Control and Data FlowAlt text \" Write Control and Data Flow Client asks Master for primary chunk which holds lease. If no primary on master: Find up to date replicas. Pick one of them to be primary, other to be secondary. Increment version number. Tells primary, secondary the version number. Master writes version number to disk. Client sends data Primary picks offset. All replicas include primary told to write appended record to offset. If primary receives all ‘yes’ reply from secondary, primary return to client. If primary failed to receives, client could restart from step 1. ","date":"2021-08-04","objectID":"/gfs/:6:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Consistency consistence - a file region is consistent if all clients will always see the same data. defined - a region is defined after a file data mutation if it is consistent and client will see what the mutation writes in its entirely. File Region State After MutationAlt text \" File Region State After Mutation Consistentcy on file/directory: Master use prefix compression to make a lookup table mapping full pathnames to metadata (etc. /d1/d2/.../dn) each master operation acquires a set of locks before runs. example: when it invokes /d1/d2/.../dn/leaf, it acquires read-lock on directory /d1/d2.../dn. Then, it acquires either a read-lock or write-lock on /d1/d2/.../dn/leaf locks are acquired by following order to prevent deadlock: level by namespace tree lexicographically within same level ","date":"2021-08-04","objectID":"/gfs/:7:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Availability Fast Recovery Both master and chunkservers are designed to restore their states and start in seconds. Chunk Replication Master Replication Its operation log and checkpoints are replicated on multiple machines. A mutation to the state is committed only after its log record has been flushed to disk locally on all replicas. Once it fails, infrastructure outside GFS starts a new master process. shadow master is used which provides read-only access when master process is down. Reference: https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf ","date":"2021-08-04","objectID":"/gfs/:8:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Discuss MapReduce research paper ","date":"2021-07-25","objectID":"/mapreduce/:0:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - MapReduce: Simplified Data Processing on Large Clusters","uri":"/mapreduce/"},{"categories":["Distribution System"],"content":"Programming Model There are two functions for MapReduce library for computations: Map function: takes a set of input key/value pairs. produces a set of intermediate key/value pairs. group all intermediate values associated with same intermediate key I and pass them to Reduce function, Reduce function: Accept an intermediate key I and a set of values (usually supplied via an iterator so that it does not cost much memory) for that key. Merges together values to form a possibly smaller set of values (typically zero or one output). Example Distributed Grep Count of URL Access Frequency Reverse Web-Link Graph Distributed Sort map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, \"1\"); reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); ","date":"2021-07-25","objectID":"/mapreduce/:1:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - MapReduce: Simplified Data Processing on Large Clusters","uri":"/mapreduce/"},{"categories":["Distribution System"],"content":"Implementation Execution overview Execution overviewAlt text \" Execution overview Mapreduce library in user program splits input files into M pieces (typically 16MB to 64MB), and start many copies of program on a cluster of machines. One of the copies of program is master and rest are workers. Master program assign works to workers. There are M map tasks and R reduce tasks. Worker who is assigned a map task reads and parses contents of input split into map function. Map function then produces intermediate key/values, which are buffered in memory. Periodically, the buffered pairs are written to the disk, partitioned into R regions. The location of regions are passed back to master. When a reduce worker is forwarded by master about regions location, it uses RPC to read buffered data from disk of different map workers. Then, it sorts data by intermediate key to eliminate multiple same keys. Reduce worker iterates over the sorted intermediate data. For each unique key, it passes set of values of the key to reduce function. The reduce function then outputs result to a file. When all map tasks and reduce tasks are done, the master worker wake up user program. MapReduce function call is finally returned. Master Data Structures It stores the state of map/reduce tasks(idle, in-progress or completed). It stores the identity of worker machine(for non-idle tasks). It stores the location and size of R intermediate file regions for completed map tasks. It sends information about intermediate file regions to workers that has in-progress reduce tasks. Fault Tolerance Work Failure: Master ping every worker periodically, and if master does not get response by a worker, master will rest all tasks that are completed or in-progress on worker machine to idle states. The completed map tasks on failed machine will be re-executed as it stored on local machine. Reduce tasks are notified with the re-execution and redirection of read machine. Master failure: Abort MapReduce function if master failed. Locality In order to save scare Network bandwidth, input data will be stored on local disks of machines. The GFS will save serveral copies (typically 3) of each split on different machines. Task Granularity master must make O(M+R) scheduling decisions and keeps O(M * R) states in memory. Backup Tasks When a MapReduce operation is close to completion, the master program backup execution of remaining in-progress tasks. Task is marked as completed whenever either the primary or the backup execution completes. Reference: https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf ","date":"2021-07-25","objectID":"/mapreduce/:2:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - MapReduce: Simplified Data Processing on Large Clusters","uri":"/mapreduce/"},{"categories":["Distribution System"],"content":"MIT 6.824 Lecture 1 Note ","date":"2021-07-24","objectID":"/mit6.824.1/:0:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"WHAT is Distribution System Multiple computer works together to serve a same service. ","date":"2021-07-24","objectID":"/mit6.824.1/:1:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"WHY Distribution System Parallelism. Fault tolerance: Avaliability - Under certain failures, we can still provide service. etc(replicate servers, one fail, and one still running). Recoverability. Non-volatile storage - HHD SSD. Replication Physical (bank transfer between two different regions etc.). Security/Isolation (separate code on different machine). ","date":"2021-07-24","objectID":"/mit6.824.1/:2:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"Consistency Put/Get: given the danger that user may get different version of data. Strong consistency: version control of get. put replicas as far as possible to prevent data loss from natural disaster. Weak consistency: Hardware is weakly ordered with respect to a synchronization model if and only if it appears sequentially consistent to all software that obey the synchronization model. ","date":"2021-07-24","objectID":"/mit6.824.1/:3:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"MapReduce Map function: Map(k,v), where k is filename and v is content. Map function split v into word. For each word w emit (w, \"1\"). Reduce function: Reduce(k, v). emit(len(v)). ","date":"2021-07-24","objectID":"/mit6.824.1/:4:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"MIT 6.824 Lecture 2 Note ","date":"2021-07-24","objectID":"/mit6.824.2/:0:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 2 RPC and Threads","uri":"/mit6.824.2/"},{"categories":["Distribution System"],"content":"Thread Feature: local stack. local registers. local sotrage. program counter. unshared code except for read-only code. Why thread: I/O Concurrency: Able to send multiple request to the network and waiting for many replies at same time. CPU Parallelism: threads can run truly in parallel. Could use multiple CPU cycles per second. Convenience: unblocking main process while doing other jobs. Thread Challenge: Race Condition multiple threads doing: n += 1, where n is shared data. Coordination Channels - way of interaction between threads. sync.Cond - good when multiple readers wait for the shared resources to be available. sync.waitGroup - good for launching a known number of goroutines and waiting for them to finish. Dead Lock example - T1 holds Lock A, T2 holds Lock B, then T1 needs Lock B and T2 needs Lock A. ","date":"2021-07-24","objectID":"/mit6.824.2/:1:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 2 RPC and Threads","uri":"/mit6.824.2/"},{"categories":["Docker"],"content":"这篇文章讨论了Docker的原理和基础用法 ","date":"2021-07-23","objectID":"/docker/:0:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"基础构架 Docker使用系统本身使用的技术进行虚拟化. ArchitectureAlt text \" Architecture Docker Daemon 运行在系统的一个应用. 持续等待Docker API 相关的请求. 建立管理 Container. Docker API 以HTTP形式建立并且返回(Restful API). Docker Client Docker CLI. 运行命令 etc. docker run, docker build… ","date":"2021-07-23","objectID":"/docker/:1:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"与传统虚拟化技术的区别 DifferenceAlt text \" Difference VM： VM 需要为每一个虚拟机创建一个用户OS，许多二进制文件和库,大概需要10GBs的. VM 启动很慢. Container: Docker通过Linux Kernel的cgroup和namespace来对container进行环境隔离. 每个Container都共享同一个kernel，所以如果一个container导致kernel崩溃后，其他所有container也会shutdown. ","date":"2021-07-23","objectID":"/docker/:2:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"容器 ContainerAlt text \" Container Docker会把每一个每一个容器赋予namespace并把容器与kernel的一个cgroup绑定. cgroup 帮助管理container里的资源(I/O speed, Memory Usage, number of process). ","date":"2021-07-23","objectID":"/docker/:3:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Docker Image 用户可以通过 docker pull image name:version 来获得 DockerFile文件. 用户可以通过 docker build \u003cpath\u003e 来创建 docker image. DockerFile: DockerFile描述了Docker Image里面的环境和应用. ADD - add files from local to containers. FROM - select a base image to build the new image on the top of. CMD - command run when container starts. RUN - Specify commands to make changes to your Image and subsequently the Containers started from this Image. This includes updating packages, installing software, adding users, creating an initial database, setting up certificates, etc. Docker Image layer: Docker Image LayerAlt text \" Docker Image Layer layer代表了DockerFile里面的一些指令. DockerFile里的一些操作比如:RUN,COPY,ADD 都会创建一个layer. 第一层是R/W层，也叫容器层. 容器过程中对文件的修改，删除只会造成容器层的改变. 后面的layer是不可修改的，是镜像层. Docker Image layer的用处在同一个image启动多个container时，layer层是共享的，所以启动非常快. ","date":"2021-07-23","objectID":"/docker/:4:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Docker Volume Docker VolumeAlt text \" Docker Volume Bind mounts: 使用方法: docker run -v \u003cHOST_PATH\u003e:\u003cCONTAINER_PATH\u003e --name \u003cname\u003e \u003cimage name:version\u003e volume会把 \u003cHOST_PATH\u003e的内容映射到container里面的\u003cContainer_PATH\u003e 当中. 作用: 对于需要不停修改的image， docker volume 让你不用重新修改DockerFile/build image, 只需要修改 docker volume/container里面的文件. 使container的数据保存在 var/lib/docker/volume 下，即使container被删除，依旧有效，保持数据持久化. volume: 使用方法: docker volume create \u003cvolume name\u003e- Create a volume. docker volume inspect \u003cvolume name\u003e- Inspect a volume. docker volume rm \u003cvolume name\u003e - Remove a volume. docker run -v \u003cDOCKER_VOL\u003e:\u003cCONTAINER_PATH\u003e --name \u003cname\u003e \u003cimage name:version\u003e 作用: 使container之间可以共享volume的内容. 保持数据持久化，只有volume被删除后内容才会消失. tmpfs(temporary file system): 只适用于linux. 会分配一块内存空间，存放特定目录底下的容器档案资料. 使用stop指令停用容器的时候，内存空间会被解放. 使用方法: docker run --name \u003cname\u003e --tmpfs \u003cCONTAINER_PATH\u003e xxxx:version 作用: 放在内存里，速度比放在disk里面更快. ","date":"2021-07-23","objectID":"/docker/:5:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Build application with Docker In DokcerFile: FROM \u003clanguage\u003e:\u003cversion\u003e # choose base image. WORKDIR \u003c/Path\u003e # relativeDir COPY . . # copy file from current dir to Docker daemon. RUN \u003ccommand\u003e # execute any commands in a new layer on top of the current image and commit the results. CMD [\"\u003cname\u003e\"] # provide defaults for an executing container 12 factor apps: Codebase 一份基准代码，多份部署. Dependencies 通过 依赖清单 ，确切地声明所有依赖项. Config 应该用环境变量 env env vars 来储存应用配置，方便不同的部署做修改. Backing services 后端服务如数据库(MySQL, MongoDB)，消息队列(RabbitMQ)，缓存系统(Memcached)作为附加资源. 通过第三方调用(AmazonS3)或者API(http://auth@api.twitter.com/)访问来管理. Build, release, run Processes Port binding Concurrency Disposability 程序是可以瞬间开启或者停止，这有利于快速、弹性的伸缩应用，迅速部署变化的 代码 或 配置 ，稳健的部署应用. Dev/prod parity Logs ln -sf /dev/stdout \u003cname\u003e.log 配合 shell script. 通过输出流把log整合到一起发送给处理程序(logplex, Fluentd)，用于查看或者存档. 把日志当成事件流(stdout) 发送到日志索引和分析系统(Splunk)或者储存库(Hadoop/Hive). 通过以上方法可以有效减少硬盘储存. Admin processes ","date":"2021-07-23","objectID":"/docker/:6:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Docker Registry docker tag \u003cname\u003e:\u003cversion\u003e \u003cuserid\u003e/\u003capp name\u003e docker login docker push \u003cREPOSITORY\u003e:\u003cversion\u003e ","date":"2021-07-23","objectID":"/docker/:7:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Docker Networking Host networking: Host networkingAlt text \" Host networking docker run --net=host nginx Bridge networking: Bridge networkingAlt text \" Bridge networking docker run -p \u003chost port\u003e:\u003ccontainer port\u003e \u003cname\u003e docker实际是在iptables做了DNAT规则，实现端口转发功能. ","date":"2021-07-23","objectID":"/docker/:8:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"常用命令 docker run - 下载并执行容器. docker start - 开始已有容器. docker stop - 关闭已有容器. docker pull - 下载容器. docker build \u003cDockerFile\u003e - 建立容器. docker exec \u003cname\u003e \u003cpath\u003e - 执行容器里的特定程序如bash. docker ps - 查看. ","date":"2021-07-23","objectID":"/docker/:9:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"常用参数 -i - run container in a interactive mode. -t - provide a tty for named container. -d - run in the background and return container ID. ","date":"2021-07-23","objectID":"/docker/:10:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Database"],"content":"这篇文章讨论了redis的基础数据结构和特点 ","date":"2021-04-22","objectID":"/redis/:0:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"数据类型 String List Linked Lists. Set collection of unique, unsorted elemnts. Hash * Zest ","date":"2021-04-22","objectID":"/redis/:1:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"数据结构 字典 存放key-value的数据结构. 使用哈希表作为底层. 用拉链法(每个bucket有一个链表)来解决哈希冲突. 包含2个哈希表，是为了在扩容时，把rehash过后的key-value放到另外一个字典上，完成交换. rehash的过程是渐进式的，每次CRUD的时候，都会顺带做一点rehash的工作. 跳跃表 Skip ListAlt text \" Skip List Zest 的底层实现. 基于多指针有序列表. 在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找. 插入速度快，相比起红黑树等不需要旋转，支持无锁操作. 插入，删除，搜索都是 O(logn). ","date":"2021-04-22","objectID":"/redis/:2:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"特点 速度快，因为数据存在内存中，并且底层kv使用hashmap来实现，查找和操作都是O(1). 单线程， 不用担心 Race Condition. ","date":"2021-04-22","objectID":"/redis/:3:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"集群 主从复制 master-slave replicationAlt text \" master-slave replication redis 提供了 复制replication功能，可以实现当master数据库更新后，自动将更新的数据同步到其他slaves数据库中. master数据库可以进行读写操作. slave数据库只能进行读操作. 优点: 容灾恢复. 读写分离，分担master读写的压力. 在同步期间，master和slave服务器是非阻塞(non-blocking)，客户端可以操作. 缺点: Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败. Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂. 哨兵Sentinel模式 sentinel replicationAlt text \" sentinel replication 哨兵是独立进程，通过向redis服务器发送request并等待response来监控运行多个redis实例. 当哨兵检测到master服务器宕机，就会将其中一个slave服务器切换成master并通知其他服务器. 优点: 主从可以自动切换，系统更健壮，可用性更高. 缺点: Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂. 集群Cluster模式 clusterAlt text \" cluster 在每台redis节点上储存不同的数据. 服务器之间互相连接，一个服务器可以访问任意一个服务器. 采用hash slot来分配节点地址. redis cluster 有16384个hash slot, 通过对每个key进行 mod 16384来决定节点的位置. 每个节点负责一部分的hash slot. ","date":"2021-04-22","objectID":"/redis/:4:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"同步 slave服务器向master服务器发送sync命令. 收到sync命令后，master服务器执行bgsave命令，用来生成rdb文件，并在一个缓冲区中记录现在开始执行的写命. bgsave执行完后，发送给slave服务器使其更新数据. master服务器再将缓冲区记录的写命令发送给从服务器，slave服务器执行完这些写命令后，此时的数据库状态便和master服务器一致了. ","date":"2021-04-22","objectID":"/redis/:5:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"持久化 redis因为是内存型rdb，为了保证数据不丢失，会在一定频率下把内存数据保存到硬盘当中. 可用在AOF文件写指令来设置保存数据的频率(etc. always, everysec, no). ","date":"2021-04-22","objectID":"/redis/:6:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["OS"],"content":"探讨内存对齐","date":"2020-05-04","objectID":"/memory_alignment/","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"这篇文章讨论了内存对齐的原理 ","date":"2020-05-04","objectID":"/memory_alignment/:0:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"内存条的结构 每个内存条一面是一个 Rank 每面Rank一般包含8个 Chips. 每个Chips包含8个 Banks. 计算机通过选择Banks的 row 和 col 来定位地址. ","date":"2020-05-04","objectID":"/memory_alignment/:1:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"结构的优点 通过对8个 Chips 进行并行parallelism操作，提高了内存访问效率. ","date":"2020-05-04","objectID":"/memory_alignment/:2:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"结构的缺陷 虽然 8 byte8 字节 物理上不是连续存在，但共用8个Chips的同一个地址. 这也导致了address只能是8的倍数%8=0. ","date":"2020-05-04","objectID":"/memory_alignment/:3:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"解决方案 CPU分两次读 如果当用户想要从不属于Chips(这里是8)倍数的地址开始读取数据，CPU会读取前后8个bytes的地址16字节来获得数据. 但这也导致了性能的降低. ","date":"2020-05-04","objectID":"/memory_alignment/:4:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"更优秀的解决方案 内存对齐 为了保持高效的运行，编译器会把不同类型/不同大小的数据安排到合适的地点并占用合适的长度. 内存对齐要求数据储存的地址是对其边界的倍数 int32 的对齐边界是 4 bytes4 字节，所以它所在的地址一定是 4 的倍数%4=0. int16 的对齐边界是 2 bytes2 字节，所以它所在的地址一定是 2 的倍数%2=0. 举例: type a struct{ A int8 B int16 C int32 D int64 } type b struct{ B int16 A int8 C int64 D int32 } 虽然上面2个struct内容相同，但data type的排序却会导致后者b struct占用较多的字节. 第一种排序占用 16 bytes16 字节. 0x00 0x01 0x02 0x03 0x04 0x05 0x06 0x07 0x08 0x09 0xa0 0xa1 0xa2 0xa3 0xa4 0xa5 8 16 16 32 32 32 32 64 64 64 64 64 64 64 64 第二种排序占用 24 bytes24 字节. 0x00 0x01 0x02 0x03 0x04 0x05 0x06 0x07 0x08 0x09 0xa0 0xa1 0xa2 0xa3 0xa4 0xa5 0xa6 0xa7 0xa8 0xa9 0xb0 0xb1 0xb2 0xb3 16 16 8 64 64 64 64 64 64 64 32 32 32 32 Golang 语言下可以用 unsafe.sizeof 来查看 struct 的大小. Reference: https://www.bilibili.com/video/BV1hv411x7we?p=3 ","date":"2020-05-04","objectID":"/memory_alignment/:5:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["Algorithm"],"content":"理解归并排序","date":"2020-03-27","objectID":"/mergesort/","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"这篇文章讨论了归并排序的原理 ","date":"2020-03-27","objectID":"/mergesort/:0:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"原理 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列. 设定两个指针，最初位置分别为两个已经排序序列的起始位置. 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置. 重复步骤 3 直到某一指针达到序列尾. 将另一序列剩下的所有元素直接复制到合并序列尾. ","date":"2020-03-27","objectID":"/mergesort/:1:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"实现 func mergeSort(arr []int) []int{ length := len(arr) if length \u003c 2{ return arr } mid := length/2 left := arr[0:mid] right := arr[mid:] return merge(mergeSort(left), mergeSort(right)) } func merge(left []int, right []int) []int{ var result []int for len(left) != 0 \u0026\u0026 len(right) != 0{ if left[0] \u003c right[0]{ result = append(result, left[0]) left = left[1:] }else{ result = append(result, right[0]) right = right[1:] } for len(left) != 0{ result = append(result, left[0]) left = left[1:] } for len(right) != 0{ result = append(result, right[0]) right = right[1:] } } return result } ","date":"2020-03-27","objectID":"/mergesort/:2:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"时间复杂度 Average Case: O(nlogn) Worst Case: O(nlogn) ","date":"2020-03-27","objectID":"/mergesort/:3:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"优点 时间复杂度稳定 ","date":"2020-03-27","objectID":"/mergesort/:4:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"缺点 虽然 Average Case 都是O(nlogn)，但QuickSort比归并排序稍快 ","date":"2020-03-27","objectID":"/mergesort/:5:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"探讨快排的原理","date":"2020-02-22","objectID":"/quicksort/","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"这篇文章讨论了快排的原理 ","date":"2020-02-22","objectID":"/quicksort/:0:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"原理 有一个大小为[0…n]的数组，我们要将它进行排序. 选中数字中的一个元素当成pivot. 把所有小于pivot的元素放在左边. 把所有大于pivot的元素放在右边. 对于pviot左侧的数组执行第二步的步骤. 对于pviot右侧的数组进行第二步的步骤. 重复以上步骤. ","date":"2020-02-22","objectID":"/quicksort/:1:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"实现 int parition(vector\u003cint\u003e\u0026 nums, int l, int r){ int pivot = nums[l]; while(l \u003c r){ while(l \u003c r \u0026\u0026 nums[r] \u003e= pivot) r--; nums[l] = nums[r]; while(l \u003c r \u0026\u0026 nums[l] \u003c= pivot) l++; nums[r] = nums[l]; } nums[l] = pivot; return l; } void quick(vector\u003cint\u003e\u0026 nums, int l, int r){ if(l \u003e r) return; int pivot = parition(nums,l,r); quick(nums,l,pivot-1); quick(nums,pivot+1,r); } ","date":"2020-02-22","objectID":"/quicksort/:2:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"时间复杂度 Average Case: O(nlogn) Worst Case: O(n2) ","date":"2020-02-22","objectID":"/quicksort/:3:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"基准的选择 固定位置 通常选择首个/最后元素作为基准. 三数取中(medium of three) 取待排序数组中间，首部和尾部中第二大的元素作为基准 实现: void getmid(vecotr\u003cint\u003e arr, int l, int r){ int mid = l + (r-l)/2; int index = 0; if(arr[l] \u003c= arr[mid] \u0026\u0026 arr[l] \u003e= arr[r]) index = l; else if(arr[r] \u003c= arr[mid] \u0026\u0026 arr[r] \u003e= arr[l]) index = r; else index = mid; //put medium value at the front swap(arr[l],arr[mid]); } ","date":"2020-02-22","objectID":"/quicksort/:4:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"缺点 如果初始序列基本为有序，则时间复杂度属于Worst Case. Pivot的选取极大影响了快排的效率. ","date":"2020-02-22","objectID":"/quicksort/:5:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"}]