[{"categories":[],"content":"TODO list ","date":"2021-08-21","objectID":"/todolist/:0:0","tags":[],"title":"TODO List","uri":"/todolist/"},{"categories":[],"content":"Paper Paxos Time, Clocks, and the Ordering of Events in a Distributed System Raft GFS HDFS MapReduce Bigtable Precolator Spanner 2PC Dynamo Cassandra Kafka Availability in Globally Distributed Storage Systems Finding a needle in Haystack: Facebook’s photo storage Mesos Pregel ","date":"2021-08-21","objectID":"/todolist/:1:0","tags":[],"title":"TODO List","uri":"/todolist/"},{"categories":[],"content":"Open Source Project Redis TIDB CockroachDB OceanBase MongoDB Kafka MySQL PostgreSQL etcd gRPC ","date":"2021-08-21","objectID":"/todolist/:2:0","tags":[],"title":"TODO List","uri":"/todolist/"},{"categories":[],"content":"Golang string slice memory alignment map function call stack method closure defer panic and recover type system interface Type Assertion reflect GPM GC Mutex semaphore reference: https://www.bilibili.com/video/BV1hv411x7we ","date":"2021-08-21","objectID":"/todolist/:3:0","tags":[],"title":"TODO List","uri":"/todolist/"},{"categories":["Memory"],"content":"Memory Model Part1 ","date":"2021-09-12","objectID":"/memorymodel1/:0:0","tags":["Memory Model"],"title":"Hardware Memory Models","uri":"/memorymodel1/"},{"categories":["Memory"],"content":"Introduction 假设我们有程序: // Thread 1 // Thread2 x = 1; while(done == 0) done = 1; print(x); 如果 thread1 和 thread2 各自在自己的processor上运行。运行完成后，程序会输出0吗？ 输出的结果取决于hardware和compiler: 如果是x86构架，就会输出1. 如果是ARM或者POWER 构架，就会输出0. 不管hardware是什么，compiler的优化都会使得程序输出0或者进行无限循环. 在多线程情况下，程序的执行结果在不同的平台有可能会造成不同的结果. 所以我们需要 memory model 来定义hardware, compiler, programming language操作内存时所遵循的模型. 本篇主要讨论的是 hardware 的 memory model. ","date":"2021-09-12","objectID":"/memorymodel1/:1:0","tags":["Memory Model"],"title":"Hardware Memory Models","uri":"/memorymodel1/"},{"categories":["Memory"],"content":"Sequential Consistency Lesile Lamport 曾提出了一个概念 sequential consistency: 每个线程内部的指令都是按照程序规定的顺序 线程执行的交错顺序可以是任意的，但是所有线程所看见的整个程序的总体执行顺序都是一样的. SC的实现模型之一: Sequential ConsistencyAlt text \" Sequential Consistency 这里的模型实现了强顺序一致性，每次当一个处理器进行 write/read 请求，这个请求就会跳转到共享内存中。 所以内存的访问顺序决定了执行的顺序，达到了顺序一致性. 但强顺序一致性会导致硬件的性能变差，所以现代硬件都偏离了SC. ","date":"2021-09-12","objectID":"/memorymodel1/:2:0","tags":["Memory Model"],"title":"Hardware Memory Models","uri":"/memorymodel1/"},{"categories":["Memory"],"content":"x86 Total Store Order x86-TSOAlt text \" x86-TSO 什么是TSO?: TSO guarantees that the sequence in which store, FLUSH, and atomic load-store instructions appear in memory for a given processor is identical to the sequence in which they were issued by the processor. x86如何实现TSO?: 所有 processors 都连接到同一个 shared memory中. 每次 write 都会加入到本地的 write queue 中，每当 一个 write 从本地的 write queue 移动到 shared memory 中， 处理器就执行一个新的指令. 每次 read 访问 shared memory 之前都会询问本地的 write queue，作用是 processor 可以先于其他 processor 看到自己的 write. 每当一个write到达 shared memory 之后，所有之后的read都会看到并且使用write的数值. Write queue 遵循了 FIFO rule. Example: message passing: 以下程序能否看见 r1 = 1, r2= 0? // Thread 1 // Thread 2 x = 1 r1 = y y = 1 r2 = x SC: no TSO: no SC 和 TSO 都保持相同的结果. x = 1 必须发生在 y = 1 之前，所以 r1 = y 不可能在 r2 = x 之前见到 y 的新值. write queue: 以下程序能否看见: r1 = 0, r2 = 0? // Thread 1 // Thread 2 x = 1 y = 1 r1 = y r2 = x SC: no TSO: yes SC 和 TSO 保持不同的结果. 在 SC 中: x = 1 或者 y = 1 必须先于 read 发生，所以 r1 和 r2 不可能同时为 0. 在 TSO 中: thread1 和 thread2 会把各自的 write 放到 write queue 中. read时，有可能 write 还没有从 write queue 放到 shared memory 中， 所以 r1 或者 r2 有可能同时为 0. 那如何保证x86遵行 Sequential Consistency呢?: Write Barrier Write barrier 保证每个线程在 read 操作之前都会把 write 置入 shared memory 当中. // Thread 1 // Thread 2 x = 1 y = 1 barrier barrier r1 = y r2 = x barrier 保证了 r1 和 r2 必须在前面2个 write 操作置入 shared memory 之后，才执行 read. Independent reads of independent writes以下程序能否看见 r1 =1, r2 = 0, r3 = 1, r4 = 0? (Thread 3 和 Thread 4 是否会见到 不同的 x 和 y) // Thread 1 // Thread 2 // Thread 3 // Thread 4 x = 1 y = 1 r1 = x r3 = y r2 = y r4 = x SC: no TSO: no 因为是 Total Store Order，所以所有 processors 都同意这个顺序. ","date":"2021-09-12","objectID":"/memorymodel1/:3:0","tags":["Memory Model"],"title":"Hardware Memory Models","uri":"/memorymodel1/"},{"categories":["Memory"],"content":"ARM/POWER Relaxed Memory Model ARM/POWER Relaxed Memory ModelAlt text \" ARM/POWER Relaxed Memory Model 每个 processor 的 read/write 都是在 local memory 中发生. 每个 write 操作都会传递给其他 processor 用于同步. ARM/POWER没有实现TSO: 每个 read 的操作可被推迟，直到需要 read 的结果. Example: message passing: 以下程序能否见到 r1 = 1, r2 = 0? // Thread 1 // Thread 2 x = 1 r1 = y y = 1 r2 = x SC: no TSO: no ARM/POWER: yes 如果 thread 1 在向 thread 2 传递 x=1 的写操作之前先传递了 y=1 的写操作，并且 thread 2 在 2个写操作传递之间完成了，就会出现上述的结果. load buffer: 以下程序能否见到 r1 = 0, r2 = 0 // Thread 1 // Thread 2 x = x y = 1 r1 = y r2 = x SC: no TSO: yes ARM: yes 如果对 x 和 y 的写操作储存到了 local memory, 但并没有传递到其他 processor 上，就会出现上述结果. ","date":"2021-09-12","objectID":"/memorymodel1/:4:0","tags":["Memory Model"],"title":"Hardware Memory Models","uri":"/memorymodel1/"},{"categories":["Memory"],"content":"Weak Ordering and Data-Race-Free Sequential Consistency 什么是 Weak ordering?: Let a synchronization model be a set of constraints on memory accesses that specify how and when synchronization needs to be done. Hardware is weakly ordered with respect to a synchronization model if and only if it appears sequentially consistent to all software that obey the synchronization model. Adve 和 Hill 提出了一种 synchronization model: data-race-free, 假设了 hardware 中除了写操作和读操作, 还有 synchronization operations 用于对 write 和 read 重新排序，从而达到顺序一致性. Data-Race Before SynchronizationAlt text \" Data-Race Before Synchronization 上面的图片中，我们无法保证两个写作操的顺序. Data-Race After SynchronizationAlt text \" Data-Race After Synchronization 通过 synchronization variable a 我们可以强制使得写操作遵循某种顺序. Data-Race Before Assigns a Intermediate ThreadAlt text \" Data-Race Before Assigns a Intermediate Thread 上面的图片中，即使使用了 synchronization variablel, 也无法消除读操作产生的 race condition. Data-Race After Assigns a Intermediate ThreadAlt text \" Data-Race After Assigns a Intermediate Thread 我们可以通过在中间加入一个 intermedaite thread 来解决这个问题. 总而言之, Adve 和 Hill 通过 data-race-free 提出 weaking ordering 就像是软件和硬件之间的协议. 为了防止 data-race 的产生，硬件通过一系列的限制来使得软件的执行结果跟遵循了Sequential Order后的执行是一模一样的. ","date":"2021-09-12","objectID":"/memorymodel1/:5:0","tags":["Memory Model"],"title":"Hardware Memory Models","uri":"/memorymodel1/"},{"categories":["Golang"],"content":"Talks about Golang Runtime ","date":"2021-09-05","objectID":"/runtime/:0:0","tags":["Golang"],"title":"Runtime","uri":"/runtime/"},{"categories":["Golang"],"content":"Runtime Golang Runtime 是 Go语言运行所需要的基础设施: 协程调度，内存分配，GC. 操作系统及CPU相关的操作的封装. pprof, trace, race 检测. map, channel, string等内置类型及反射的发现. ","date":"2021-09-05","objectID":"/runtime/:1:0","tags":["Golang"],"title":"Runtime","uri":"/runtime/"},{"categories":["Golang"],"content":"调度 Goroutine 实现: 线程的运行是通过调度队列切换不同的执行流，而goroutine就是执行流. Goroutine的结构体: type g struct { goid int64 // 协程的id status uint32 // 协程的状态 stack struct{ lo uintptr // 协程拥有的栈低位 hi uintptr // 协程拥有的栈高位 } sched gobuf // 切换时保存的上下文 startfunc uintptr // 程序地址 } type gobuf struct { sp uintptr // 栈指针位置 pc uintptr // 运行到的程序位置 } 每个 go func 都会编译成 runtime.newproc 函数, 最终有一个 runtime.g 对象放入调度队列. 函数的指针设置在 runtime.g 的 startfunc 字段, 参数会在 newproc 函数里拷贝到 stack 中, sched 用于保存协程切换时的 pc 位置和栈位置. Processor的结构体: type p struct { id int32 status uint32 m muintpr // 与之关联的m mcache *mcache // per-p分配的cache runqhead uint32 runqtail uint32 runnext guintpr gFree struct { gList n int32 } palloc persistentAlloc // per-P, 用于分配一些runtime里的特殊结构. gcw gcWork } machine的结构体: type m struct { g0 *g // 每个m绑定的g procid uint64 // 底层的线程id tls [6]uint64 // 传给FS寄存器的线程局部变量 mstartfn func() // m启动时的函数，传给clone curg *g // 现在运行的代码的g p puintptr // 在运行代码时绑定的p id int64 spinning bool // m找不到可运行的g，spin mcache *mcache // 运行代码时绑定的p中的mcache lockedg guintptr // 是否与某个g一直绑定 } ","date":"2021-09-05","objectID":"/runtime/:2:0","tags":["Golang"],"title":"Runtime","uri":"/runtime/"},{"categories":["Golang"],"content":"GC 三色标记法: 一开始所有对象都是白色 从root开始标记，把所有可达到的对象标记为灰色 从灰色对象集合取出对象并标记灰色，把自己标记为黑色 重复第三步直到灰色集合是空 剩下的白色就是不可到达的对象 写屏障: 万一出现漏标记对象的现象：A指向nil并被标记，B指向C，A被标记后指向C，B指向nil，这样B被标记的时候就忽略了C，所以我们有了写屏障: 强三色不变性 — 黑色对象不会指向白色对象，只会指向灰色对象或者黑色对象 弱三色不变性 — 黑色对象指向的白色对象必须包含一条从灰色对象经由多个白色对象的可达路径 插入写屏障: 如果两个对象之间新建立引用，那么引用(黑色)指向的对象就会被标记为灰色以满足强三色不变性，这是一种相对保守的屏障技术。 删除屏障: 如果一个灰色对象指向一个白色对象的引用被删除，那么在删除之前写屏障检测到内存变化，就会把这个白色对象标灰 ","date":"2021-09-05","objectID":"/runtime/:3:0","tags":["Golang"],"title":"Runtime","uri":"/runtime/"},{"categories":["Distribution System"],"content":"Discuss HDFS research paper ","date":"2021-09-05","objectID":"/hdfs/:0:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - HDFS","uri":"/hdfs/"},{"categories":["Distribution System"],"content":"What is HDFS HDFS is a file system component of Hadoop, which store metadata and application data separately. ","date":"2021-09-05","objectID":"/hdfs/:1:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - HDFS","uri":"/hdfs/"},{"categories":["Distribution System"],"content":"Why HDFS HDFS is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. ","date":"2021-09-05","objectID":"/hdfs/:2:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - HDFS","uri":"/hdfs/"},{"categories":["Distribution System"],"content":"Architecture NameNode: maintain the namespace tree and mapping of file blocks(phyiscal address) to DataNodes. namesapce is a hierarchy of files and directories, and is stored in RAM. Files and directories are represented by Inode, which record permission, modification and access time. DataNode: Two files: one store application data and another one store metadata. File content is split into large blocks and each block is replicated at multiple DataNodes. Intially with no namespace ID, and able to join cluster and receive the cluster’s namespace ID. After joining a NameNode for a first time, DataNode registers a unique storage ID. Each time DataNode connects to a NameNode, namespace ID and software version will be verified, DataNode will be shutting down if either of them match. Send Hearbeat message periodically to NameNode to confirm that DataNode is operating. HDFS Client: User application that accesses file system. Process of reading data: Asks NameNode for list of DataNodes. Connect to a DataNode and request the transfer of the desired block. Pipelines data to the chosen DataNodes. Confirm the creation of the block replicas to the NameNode. Client writing dataAlt text \" Client writing data ","date":"2021-09-05","objectID":"/hdfs/:3:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - HDFS","uri":"/hdfs/"},{"categories":["Distribution System"],"content":"Block Placement HDFS replica placement policy: No Datanode contains more than one replica of any block. No rack contains more than two replicas of the same block. Cluster TopologyAlt text \" Cluster Topology ","date":"2021-09-05","objectID":"/hdfs/:4:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - HDFS","uri":"/hdfs/"},{"categories":["Distribution System"],"content":"Consistency Write: HDFS maintains consistency by allowing only single client at a time to write the file using Write Leases, but it does prevent other clients from reading the file. Read: Data written to the current block is not guaranteed to be visual to other clients, which read the content of the file. Only when the writing client concluded its write operation and closed the file, the new content is visible for sure. ","date":"2021-09-05","objectID":"/hdfs/:5:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - HDFS","uri":"/hdfs/"},{"categories":["Distribution System"],"content":"Availability A DataNode stores checksums in a metadata file separate from the block’s data file. When HDFS reads a file, each block’s data and checksums are shipped to the client. The client then can fetch a difference replica once the checksum is not right. However, HDFS does not support the case of multiple correlated failures (ie. three failed DataNodes with same HDFS block) ","date":"2021-09-05","objectID":"/hdfs/:6:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - HDFS","uri":"/hdfs/"},{"categories":["Distribution System"],"content":"Parititon tolerance When write, Client will wait for ACK from replicas, and the transaction will be successful if the number of DataNodes that acknowledge the reception of all the packages is equal or greater than minimum number of replicas. The NameNode endeavors to ensure that each block always has the intended number of replicas. The NameNode make sures that not all replicas of a block are located on one rack using block placement policy. ","date":"2021-09-05","objectID":"/hdfs/:7:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - HDFS","uri":"/hdfs/"},{"categories":["Distribution System"],"content":"Why Raft It provides a consensus algorithm allowing a collection of machines to work as a group that can survive the failures of some of its members. ","date":"2021-08-10","objectID":"/raft/:1:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Features Strong leader logs only flow from leader to others. Leader election Raft uses randomized timers to elect leaders. Membership changes Raft uses joint consensus approach where two different configurations overlap largely during transitions. ","date":"2021-08-10","objectID":"/raft/:2:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Replicated state machines State MachineAlt text \" State Machine The state machines process identical sequences of commands from the logs. Each servers stores a replicated log. ","date":"2021-08-10","objectID":"/raft/:3:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Raft Consensus algorithm It ensures safety under all non-Byzantine conditions, including: network delays partitions packet loss duplication reordering It decomposes consensus problem into three subproblems: Leader election - A leader must br chosen when an exisiting leader fails (Section 5.2). Log replication - the leader must accept log entries from clients and replicates them across the cluster (Section 5.3). Safety - see figure below. SafetyAlt text \" Safety ","date":"2021-08-10","objectID":"/raft/:4:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Basic Structure Each server has one of state: Candidate - used to elect a new leader. Follwer - simply respond to rquests from leaders and candidates. Leader - handles all client requests(follower will redirect request to leader). State ChangeAlt text \" State Change ","date":"2021-08-10","objectID":"/raft/:5:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"RPCs Two type of RPC: RequestVote RPCs - initiated by candidates. AppendEntries RPCs - initiated by leader to replicate log and send heartbeat. ","date":"2021-08-10","objectID":"/raft/:6:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"Leader election When server start up, they begin as followers. Leader send periodic heartbeats(append with no log entries) to all followers. When a follower receive no message over a period (election timeout), it starts a election: Follower increments its term and becomes candidate. Candidate vote itself and issues requestVote RPCs. Candidate mantains its state until: It wins election. Another server becoms a leader. No winner over a period of time. When a candidate receives RPCs from other candidates with \u003c term, it rejects. When a candidate receives RPCs from other candidates with \u003e= term, it becomes follower. ","date":"2021-08-10","objectID":"/raft/:7:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - Raft","uri":"/raft/"},{"categories":["Distribution System"],"content":"记录下做tinykv的想法 ","date":"2021-08-06","objectID":"/pingcap-tinykv/:0:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"高可用分布式储存系统必要的需求: 合理的副本数量 分配副本到不同的机器上 加入新节点后，要将其他节点上的副本迁移过来 节点下线后，要将下线节点的数据迁移 ","date":"2021-08-06","objectID":"/pingcap-tinykv/:1:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"高可用分布式储存系统可选的需求: 维持每个集群的 Leader 分布均匀 维持每个节点的储存容量均匀 维持访问热点分布均匀 管理 Balance 的速度，避免影响服务 管理节点状态，包括上线/下线节点，以及自动下线失效节点 ","date":"2021-08-06","objectID":"/pingcap-tinykv/:2:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"Project 1 Create DB first. CF is a key namespace where seperates same keys in different key namesapce. DB is based on badger. * It uses MVCC. * It runs transactions concurrently. * It has serializable snapshot isolation guarantees. * It uses LSM tree with value log to seperate keys from values. Each read/write opens a new transcation. Discard() after each transaction ","date":"2021-08-06","objectID":"/pingcap-tinykv/:3:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"Project 2 A: Initialize the new raft. Handle step to deal about state change and message received/sent. Each time a follower/candidate encounters election timeout, it becomes a candidate to start a new election, the term + 1. voter use first-come-first-served rule, which only votes once in a single term. Use randomnized election timeout to prevent conflict that many followers become candiate at the same time. Remeber to clean the votes map for each term raft becomes a candidate B: ","date":"2021-08-06","objectID":"/pingcap-tinykv/:4:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"Reference https://pingcap.com/blog-cn/tidb-internal-1/ https://pingcap.com/blog-cn/tidb-internal-3/. ","date":"2021-08-06","objectID":"/pingcap-tinykv/:5:0","tags":["Distribution System"],"title":"Pingcap Project - Tinykv","uri":"/pingcap-tinykv/"},{"categories":["Distribution System"],"content":"Discuss GFS research paper ","date":"2021-08-04","objectID":"/gfs/:0:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"What is GFS A distributed file system for large distributed data-intensive application. ","date":"2021-08-04","objectID":"/gfs/:1:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Why GFS Big, fast Global data - visit data across many data center. Sharding - split data into serveral servers for parallelism. Automatic recovery - recover data from failure automatically. ","date":"2021-08-04","objectID":"/gfs/:2:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Big Storage is hard Performance -\u003e sharding(split data into serveral severs in order to operate in parallel) Faults -\u003e Tolerance Tolerance -\u003e replication Replication -\u003e Inconsistency Consistency -\u003e Low performance ","date":"2021-08-04","objectID":"/gfs/:3:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Structure of GFS GFS ArchitectureAlt text \" GFS Architecture C1, C2, C3 ... Cn | | | Master | | | Chunk server 1 Chunk server 2 Chunk server 3 ... Chunk server: Responsible for reading data. Usually has three copies of same chunk on other chunkservers. Master data: Responsible for writing data and controll chunkservers. Store file, chunk namespaces, mapping of files to chunks, locations of each chunks' replcias in RAM. file name -\u003e array of chunk handles hanlde -\u003e list of chunkservers version number primary(what chunkserver is primary, what are copies) lease expiration Store log(operation such as write, read), checkpoint(version number) in disk. ","date":"2021-08-04","objectID":"/gfs/:4:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Read name, offset -\u003e Master Master send chunk handle, list of servers Client read -\u003e Chunkserver Chunkserver return data -\u003e client ","date":"2021-08-04","objectID":"/gfs/:5:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Write Write Control and Data FlowAlt text \" Write Control and Data Flow Client asks Master for primary chunk which holds lease. If no primary on master: Find up to date replicas. Pick one of them to be primary, other to be secondary. Increment version number. Tells primary, secondary the version number. Master writes version number to disk. Client sends data Primary picks offset. All replicas include primary told to write appended record to offset. If primary receives all ‘yes’ reply from secondary, primary return to client. If primary failed to receives, client could restart from step 1. ","date":"2021-08-04","objectID":"/gfs/:6:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Consistency consistence - a file region is consistent if all clients will always see the same data. defined - a region is defined after a file data mutation if it is consistent and client will see what the mutation writes in its entirely. File Region State After MutationAlt text \" File Region State After Mutation Consistentcy on file/directory: Master use prefix compression to make a lookup table mapping full pathnames to metadata (etc. /d1/d2/.../dn) each master operation acquires a set of locks before runs. example: when it invokes /d1/d2/.../dn/leaf, it acquires read-lock on directory /d1/d2.../dn. Then, it acquires either a read-lock or write-lock on /d1/d2/.../dn/leaf locks are acquired by following order to prevent deadlock: level by namespace tree lexicographically within same level ","date":"2021-08-04","objectID":"/gfs/:7:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Availability Fast Recovery Both master and chunkservers are designed to restore their states and start in seconds. Chunk Replication Master Replication Its operation log and checkpoints are replicated on multiple machines. A mutation to the state is committed only after its log record has been flushed to disk locally on all replicas. Once it fails, infrastructure outside GFS starts a new master process. shadow master is used which provides read-only access when master process is down. Reference: https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf ","date":"2021-08-04","objectID":"/gfs/:8:0","tags":["Research Paper","Distributed file system"],"title":"Research Paper - GFS","uri":"/gfs/"},{"categories":["Distribution System"],"content":"Discuss MapReduce research paper ","date":"2021-07-25","objectID":"/mapreduce/:0:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - MapReduce: Simplified Data Processing on Large Clusters","uri":"/mapreduce/"},{"categories":["Distribution System"],"content":"Programming Model There are two functions for MapReduce library for computations: Map function: takes a set of input key/value pairs. produces a set of intermediate key/value pairs. group all intermediate values associated with same intermediate key I and pass them to Reduce function, Reduce function: Accept an intermediate key I and a set of values (usually supplied via an iterator so that it does not cost much memory) for that key. Merges together values to form a possibly smaller set of values (typically zero or one output). Example Distributed Grep Count of URL Access Frequency Reverse Web-Link Graph Distributed Sort map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, \"1\"); reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); ","date":"2021-07-25","objectID":"/mapreduce/:1:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - MapReduce: Simplified Data Processing on Large Clusters","uri":"/mapreduce/"},{"categories":["Distribution System"],"content":"Implementation Execution overview Execution overviewAlt text \" Execution overview Mapreduce library in user program splits input files into M pieces (typically 16MB to 64MB), and start many copies of program on a cluster of machines. One of the copies of program is master and rest are workers. Master program assign works to workers. There are M map tasks and R reduce tasks. Worker who is assigned a map task reads and parses contents of input split into map function. Map function then produces intermediate key/values, which are buffered in memory. Periodically, the buffered pairs are written to the disk, partitioned into R regions. The location of regions are passed back to master. When a reduce worker is forwarded by master about regions location, it uses RPC to read buffered data from disk of different map workers. Then, it sorts data by intermediate key to eliminate multiple same keys. Reduce worker iterates over the sorted intermediate data. For each unique key, it passes set of values of the key to reduce function. The reduce function then outputs result to a file. When all map tasks and reduce tasks are done, the master worker wake up user program. MapReduce function call is finally returned. Master Data Structures It stores the state of map/reduce tasks(idle, in-progress or completed). It stores the identity of worker machine(for non-idle tasks). It stores the location and size of R intermediate file regions for completed map tasks. It sends information about intermediate file regions to workers that has in-progress reduce tasks. Fault Tolerance Work Failure: Master ping every worker periodically, and if master does not get response by a worker, master will rest all tasks that are completed or in-progress on worker machine to idle states. The completed map tasks on failed machine will be re-executed as it stored on local machine. Reduce tasks are notified with the re-execution and redirection of read machine. Master failure: Abort MapReduce function if master failed. Locality In order to save scare Network bandwidth, input data will be stored on local disks of machines. The GFS will save serveral copies (typically 3) of each split on different machines. Task Granularity master must make O(M+R) scheduling decisions and keeps O(M * R) states in memory. Backup Tasks When a MapReduce operation is close to completion, the master program backup execution of remaining in-progress tasks. Task is marked as completed whenever either the primary or the backup execution completes. Reference: https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf ","date":"2021-07-25","objectID":"/mapreduce/:2:0","tags":["Research Paper","Distribution System"],"title":"Research Paper - MapReduce: Simplified Data Processing on Large Clusters","uri":"/mapreduce/"},{"categories":["Distribution System"],"content":"MIT 6.824 Lecture 1 Note ","date":"2021-07-24","objectID":"/mit6.824.1/:0:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"WHAT is Distribution System Multiple computer works together to serve a same service. ","date":"2021-07-24","objectID":"/mit6.824.1/:1:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"WHY Distribution System Parallelism. Fault tolerance: Avaliability - Under certain failures, we can still provide service. etc(replicate servers, one fail, and one still running). Recoverability. Non-volatile storage - HHD SSD. Replication Physical (bank transfer between two different regions etc.). Security/Isolation (separate code on different machine). ","date":"2021-07-24","objectID":"/mit6.824.1/:2:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"Consistency Put/Get: given the danger that user may get different version of data. Strong consistency: version control of get. put replicas as far as possible to prevent data loss from natural disaster. Weak consistency: Hardware is weakly ordered with respect to a synchronization model if and only if it appears sequentially consistent to all software that obey the synchronization model. ","date":"2021-07-24","objectID":"/mit6.824.1/:3:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"MapReduce Map function: Map(k,v), where k is filename and v is content. Map function split v into word. For each word w emit (w, \"1\"). Reduce function: Reduce(k, v). emit(len(v)). ","date":"2021-07-24","objectID":"/mit6.824.1/:4:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 1 Introduction","uri":"/mit6.824.1/"},{"categories":["Distribution System"],"content":"MIT 6.824 Lecture 2 Note ","date":"2021-07-24","objectID":"/mit6.824.2/:0:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 2 RPC and Threads","uri":"/mit6.824.2/"},{"categories":["Distribution System"],"content":"Thread Feature: local stack. local registers. local sotrage. program counter. unshared code except for read-only code. Why thread: I/O Concurrency: Able to send multiple request to the network and waiting for many replies at same time. CPU Parallelism: threads can run truly in parallel. Could use multiple CPU cycles per second. Convenience: unblocking main process while doing other jobs. Thread Challenge: Race Condition multiple threads doing: n += 1, where n is shared data. Coordination Channels - way of interaction between threads. sync.Cond - good when multiple readers wait for the shared resources to be available. sync.waitGroup - good for launching a known number of goroutines and waiting for them to finish. Dead Lock example - T1 holds Lock A, T2 holds Lock B, then T1 needs Lock B and T2 needs Lock A. ","date":"2021-07-24","objectID":"/mit6.824.2/:1:0","tags":["Distribution System","Course"],"title":"MIT 6.824 - Lecture 2 RPC and Threads","uri":"/mit6.824.2/"},{"categories":["Docker"],"content":"这篇文章讨论了Docker的原理和基础用法 ","date":"2021-07-23","objectID":"/docker/:0:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"基础构架 Docker使用系统本身使用的技术进行虚拟化. ArchitectureAlt text \" Architecture Docker Daemon 运行在系统的一个应用. 持续等待Docker API 相关的请求. 建立管理 Container. Docker API 以HTTP形式建立并且返回(Restful API). Docker Client Docker CLI. 运行命令 etc. docker run, docker build… ","date":"2021-07-23","objectID":"/docker/:1:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"与传统虚拟化技术的区别 DifferenceAlt text \" Difference VM： VM 需要为每一个虚拟机创建一个用户OS，许多二进制文件和库,大概需要10GBs的. VM 启动很慢. Container: Docker通过Linux Kernel的cgroup和namespace来对container进行环境隔离. 每个Container都共享同一个kernel，所以如果一个container导致kernel崩溃后，其他所有container也会shutdown. ","date":"2021-07-23","objectID":"/docker/:2:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"容器 ContainerAlt text \" Container Docker会把每一个每一个容器赋予namespace并把容器与kernel的一个cgroup绑定. cgroup 帮助管理container里的资源(I/O speed, Memory Usage, number of process). ","date":"2021-07-23","objectID":"/docker/:3:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Docker Image 用户可以通过 docker pull image name:version 来获得 DockerFile文件. 用户可以通过 docker build \u003cpath\u003e 来创建 docker image. DockerFile: DockerFile描述了Docker Image里面的环境和应用. ADD - add files from local to containers. FROM - select a base image to build the new image on the top of. CMD - command run when container starts. RUN - Specify commands to make changes to your Image and subsequently the Containers started from this Image. This includes updating packages, installing software, adding users, creating an initial database, setting up certificates, etc. Docker Image layer: Docker Image LayerAlt text \" Docker Image Layer layer代表了DockerFile里面的一些指令. DockerFile里的一些操作比如:RUN,COPY,ADD 都会创建一个layer. 第一层是R/W层，也叫容器层. 容器过程中对文件的修改，删除只会造成容器层的改变. 后面的layer是不可修改的，是镜像层. Docker Image layer的用处在同一个image启动多个container时，layer层是共享的，所以启动非常快. ","date":"2021-07-23","objectID":"/docker/:4:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Docker Volume Docker VolumeAlt text \" Docker Volume Bind mounts: 使用方法: docker run -v \u003cHOST_PATH\u003e:\u003cCONTAINER_PATH\u003e --name \u003cname\u003e \u003cimage name:version\u003e volume会把 \u003cHOST_PATH\u003e的内容映射到container里面的\u003cContainer_PATH\u003e 当中. 作用: 对于需要不停修改的image， docker volume 让你不用重新修改DockerFile/build image, 只需要修改 docker volume/container里面的文件. 使container的数据保存在 var/lib/docker/volume 下，即使container被删除，依旧有效，保持数据持久化. volume: 使用方法: docker volume create \u003cvolume name\u003e- Create a volume. docker volume inspect \u003cvolume name\u003e- Inspect a volume. docker volume rm \u003cvolume name\u003e - Remove a volume. docker run -v \u003cDOCKER_VOL\u003e:\u003cCONTAINER_PATH\u003e --name \u003cname\u003e \u003cimage name:version\u003e 作用: 使container之间可以共享volume的内容. 保持数据持久化，只有volume被删除后内容才会消失. tmpfs(temporary file system): 只适用于linux. 会分配一块内存空间，存放特定目录底下的容器档案资料. 使用stop指令停用容器的时候，内存空间会被解放. 使用方法: docker run --name \u003cname\u003e --tmpfs \u003cCONTAINER_PATH\u003e xxxx:version 作用: 放在内存里，速度比放在disk里面更快. ","date":"2021-07-23","objectID":"/docker/:5:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Build application with Docker In DokcerFile: FROM \u003clanguage\u003e:\u003cversion\u003e # choose base image. WORKDIR \u003c/Path\u003e # relativeDir COPY . . # copy file from current dir to Docker daemon. RUN \u003ccommand\u003e # execute any commands in a new layer on top of the current image and commit the results. CMD [\"\u003cname\u003e\"] # provide defaults for an executing container 12 factor apps: Codebase 一份基准代码，多份部署. Dependencies 通过 依赖清单 ，确切地声明所有依赖项. Config 应该用环境变量 env env vars 来储存应用配置，方便不同的部署做修改. Backing services 后端服务如数据库(MySQL, MongoDB)，消息队列(RabbitMQ)，缓存系统(Memcached)作为附加资源. 通过第三方调用(AmazonS3)或者API(http://auth@api.twitter.com/)访问来管理. Build, release, run Processes Port binding Concurrency Disposability 程序是可以瞬间开启或者停止，这有利于快速、弹性的伸缩应用，迅速部署变化的 代码 或 配置 ，稳健的部署应用. Dev/prod parity Logs ln -sf /dev/stdout \u003cname\u003e.log 配合 shell script. 通过输出流把log整合到一起发送给处理程序(logplex, Fluentd)，用于查看或者存档. 把日志当成事件流(stdout) 发送到日志索引和分析系统(Splunk)或者储存库(Hadoop/Hive). 通过以上方法可以有效减少硬盘储存. Admin processes ","date":"2021-07-23","objectID":"/docker/:6:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Docker Registry docker tag \u003cname\u003e:\u003cversion\u003e \u003cuserid\u003e/\u003capp name\u003e docker login docker push \u003cREPOSITORY\u003e:\u003cversion\u003e ","date":"2021-07-23","objectID":"/docker/:7:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"Docker Networking Host networking: Host networkingAlt text \" Host networking docker run --net=host nginx Bridge networking: Bridge networkingAlt text \" Bridge networking docker run -p \u003chost port\u003e:\u003ccontainer port\u003e \u003cname\u003e docker实际是在iptables做了DNAT规则，实现端口转发功能. ","date":"2021-07-23","objectID":"/docker/:8:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"常用命令 docker run - 下载并执行容器. docker start - 开始已有容器. docker stop - 关闭已有容器. docker pull - 下载容器. docker build \u003cDockerFile\u003e - 建立容器. docker exec \u003cname\u003e \u003cpath\u003e - 执行容器里的特定程序如bash. docker ps - 查看. ","date":"2021-07-23","objectID":"/docker/:9:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Docker"],"content":"常用参数 -i - run container in a interactive mode. -t - provide a tty for named container. -d - run in the background and return container ID. ","date":"2021-07-23","objectID":"/docker/:10:0","tags":["Container","Golang"],"title":"Docker","uri":"/docker/"},{"categories":["Database"],"content":"这篇文章讨论了redis的基础数据结构和特点 ","date":"2021-04-22","objectID":"/redis/:0:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"数据类型 String List Linked Lists. Set collection of unique, unsorted elemnts. Hash * Zest ","date":"2021-04-22","objectID":"/redis/:1:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"数据结构 字典 存放key-value的数据结构. 使用哈希表作为底层. 用拉链法(每个bucket有一个链表)来解决哈希冲突. 包含2个哈希表，是为了在扩容时，把rehash过后的key-value放到另外一个字典上，完成交换. rehash的过程是渐进式的，每次CRUD的时候，都会顺带做一点rehash的工作. 跳跃表 Skip ListAlt text \" Skip List Zest 的底层实现. 基于多指针有序列表. 在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找. 插入速度快，相比起红黑树等不需要旋转，支持无锁操作. 插入，删除，搜索都是 O(logn). ","date":"2021-04-22","objectID":"/redis/:2:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"特点 速度快，因为数据存在内存中，并且底层kv使用hashmap来实现，查找和操作都是O(1). 单线程， 不用担心 Race Condition. ","date":"2021-04-22","objectID":"/redis/:3:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"集群 主从复制 master-slave replicationAlt text \" master-slave replication redis 提供了 复制replication功能，可以实现当master数据库更新后，自动将更新的数据同步到其他slaves数据库中. master数据库可以进行读写操作. slave数据库只能进行读操作. 优点: 容灾恢复. 读写分离，分担master读写的压力. 在同步期间，master和slave服务器是非阻塞(non-blocking)，客户端可以操作. 缺点: Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败. Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂. 哨兵Sentinel模式 sentinel replicationAlt text \" sentinel replication 哨兵是独立进程，通过向redis服务器发送request并等待response来监控运行多个redis实例. 当哨兵检测到master服务器宕机，就会将其中一个slave服务器切换成master并通知其他服务器. 优点: 主从可以自动切换，系统更健壮，可用性更高. 缺点: Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂. 集群Cluster模式 clusterAlt text \" cluster 在每台redis节点上储存不同的数据. 服务器之间互相连接，一个服务器可以访问任意一个服务器. 采用hash slot来分配节点地址. redis cluster 有16384个hash slot, 通过对每个key进行 mod 16384来决定节点的位置. 每个节点负责一部分的hash slot. ","date":"2021-04-22","objectID":"/redis/:4:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"同步 slave服务器向master服务器发送sync命令. 收到sync命令后，master服务器执行bgsave命令，用来生成rdb文件，并在一个缓冲区中记录现在开始执行的写命. bgsave执行完后，发送给slave服务器使其更新数据. master服务器再将缓冲区记录的写命令发送给从服务器，slave服务器执行完这些写命令后，此时的数据库状态便和master服务器一致了. ","date":"2021-04-22","objectID":"/redis/:5:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["Database"],"content":"持久化 redis因为是内存型rdb，为了保证数据不丢失，会在一定频率下把内存数据保存到硬盘当中. 可用在AOF文件写指令来设置保存数据的频率(etc. always, everysec, no). ","date":"2021-04-22","objectID":"/redis/:6:0","tags":["Database","NoSQL"],"title":"Redis","uri":"/redis/"},{"categories":["OS"],"content":"探讨内存对齐","date":"2020-05-04","objectID":"/memory_alignment/","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"这篇文章讨论了内存对齐的原理 ","date":"2020-05-04","objectID":"/memory_alignment/:0:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"内存条的结构 每个内存条一面是一个 Rank 每面Rank一般包含8个 Chips. 每个Chips包含8个 Banks. 计算机通过选择Banks的 row 和 col 来定位地址. ","date":"2020-05-04","objectID":"/memory_alignment/:1:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"结构的优点 通过对8个 Chips 进行并行parallelism操作，提高了内存访问效率. ","date":"2020-05-04","objectID":"/memory_alignment/:2:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"结构的缺陷 虽然 8 byte8 字节 物理上不是连续存在，但共用8个Chips的同一个地址. 这也导致了address只能是8的倍数%8=0. ","date":"2020-05-04","objectID":"/memory_alignment/:3:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"解决方案 CPU分两次读 如果当用户想要从不属于Chips(这里是8)倍数的地址开始读取数据，CPU会读取前后8个bytes的地址16字节来获得数据. 但这也导致了性能的降低. ","date":"2020-05-04","objectID":"/memory_alignment/:4:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["OS"],"content":"更优秀的解决方案 内存对齐 为了保持高效的运行，编译器会把不同类型/不同大小的数据安排到合适的地点并占用合适的长度. 内存对齐要求数据储存的地址是对其边界的倍数 int32 的对齐边界是 4 bytes4 字节，所以它所在的地址一定是 4 的倍数%4=0. int16 的对齐边界是 2 bytes2 字节，所以它所在的地址一定是 2 的倍数%2=0. 举例: type a struct{ A int8 B int16 C int32 D int64 } type b struct{ B int16 A int8 C int64 D int32 } 虽然上面2个struct内容相同，但data type的排序却会导致后者b struct占用较多的字节. 第一种排序占用 16 bytes16 字节. 0x00 0x01 0x02 0x03 0x04 0x05 0x06 0x07 0x08 0x09 0xa0 0xa1 0xa2 0xa3 0xa4 0xa5 8 16 16 32 32 32 32 64 64 64 64 64 64 64 64 第二种排序占用 24 bytes24 字节. 0x00 0x01 0x02 0x03 0x04 0x05 0x06 0x07 0x08 0x09 0xa0 0xa1 0xa2 0xa3 0xa4 0xa5 0xa6 0xa7 0xa8 0xa9 0xb0 0xb1 0xb2 0xb3 16 16 8 64 64 64 64 64 64 64 32 32 32 32 Golang 语言下可以用 unsafe.sizeof 来查看 struct 的大小. Reference: https://www.bilibili.com/video/BV1hv411x7we?p=3 ","date":"2020-05-04","objectID":"/memory_alignment/:5:0","tags":["OS","Memory"],"title":"OS - Memory Alignment","uri":"/memory_alignment/"},{"categories":["Algorithm"],"content":"理解归并排序","date":"2020-03-27","objectID":"/mergesort/","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"这篇文章讨论了归并排序的原理 ","date":"2020-03-27","objectID":"/mergesort/:0:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"原理 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列. 设定两个指针，最初位置分别为两个已经排序序列的起始位置. 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置. 重复步骤 3 直到某一指针达到序列尾. 将另一序列剩下的所有元素直接复制到合并序列尾. ","date":"2020-03-27","objectID":"/mergesort/:1:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"实现 func mergeSort(arr []int) []int{ length := len(arr) if length \u003c 2{ return arr } mid := length/2 left := arr[0:mid] right := arr[mid:] return merge(mergeSort(left), mergeSort(right)) } func merge(left []int, right []int) []int{ var result []int for len(left) != 0 \u0026\u0026 len(right) != 0{ if left[0] \u003c right[0]{ result = append(result, left[0]) left = left[1:] }else{ result = append(result, right[0]) right = right[1:] } for len(left) != 0{ result = append(result, left[0]) left = left[1:] } for len(right) != 0{ result = append(result, right[0]) right = right[1:] } } return result } ","date":"2020-03-27","objectID":"/mergesort/:2:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"时间复杂度 Average Case: O(nlogn) Worst Case: O(nlogn) ","date":"2020-03-27","objectID":"/mergesort/:3:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"优点 时间复杂度稳定 ","date":"2020-03-27","objectID":"/mergesort/:4:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"缺点 虽然 Average Case 都是O(nlogn)，但QuickSort比归并排序稍快 ","date":"2020-03-27","objectID":"/mergesort/:5:0","tags":["Sort"],"title":"Algorithm - MergeSort","uri":"/mergesort/"},{"categories":["Algorithm"],"content":"探讨快排的原理","date":"2020-02-22","objectID":"/quicksort/","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"这篇文章讨论了快排的原理 ","date":"2020-02-22","objectID":"/quicksort/:0:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"原理 有一个大小为[0…n]的数组，我们要将它进行排序. 选中数字中的一个元素当成pivot. 把所有小于pivot的元素放在左边. 把所有大于pivot的元素放在右边. 对于pviot左侧的数组执行第二步的步骤. 对于pviot右侧的数组进行第二步的步骤. 重复以上步骤. ","date":"2020-02-22","objectID":"/quicksort/:1:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"实现 int parition(vector\u003cint\u003e\u0026 nums, int l, int r){ int pivot = nums[l]; while(l \u003c r){ while(l \u003c r \u0026\u0026 nums[r] \u003e= pivot) r--; nums[l] = nums[r]; while(l \u003c r \u0026\u0026 nums[l] \u003c= pivot) l++; nums[r] = nums[l]; } nums[l] = pivot; return l; } void quick(vector\u003cint\u003e\u0026 nums, int l, int r){ if(l \u003e r) return; int pivot = parition(nums,l,r); quick(nums,l,pivot-1); quick(nums,pivot+1,r); } ","date":"2020-02-22","objectID":"/quicksort/:2:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"时间复杂度 Average Case: O(nlogn) Worst Case: O(n2) ","date":"2020-02-22","objectID":"/quicksort/:3:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"基准的选择 固定位置 通常选择首个/最后元素作为基准. 三数取中(medium of three) 取待排序数组中间，首部和尾部中第二大的元素作为基准 实现: void getmid(vecotr\u003cint\u003e arr, int l, int r){ int mid = l + (r-l)/2; int index = 0; if(arr[l] \u003c= arr[mid] \u0026\u0026 arr[l] \u003e= arr[r]) index = l; else if(arr[r] \u003c= arr[mid] \u0026\u0026 arr[r] \u003e= arr[l]) index = r; else index = mid; //put medium value at the front swap(arr[l],arr[mid]); } ","date":"2020-02-22","objectID":"/quicksort/:4:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"},{"categories":["Algorithm"],"content":"缺点 如果初始序列基本为有序，则时间复杂度属于Worst Case. Pivot的选取极大影响了快排的效率. ","date":"2020-02-22","objectID":"/quicksort/:5:0","tags":["Sort"],"title":"Algorithm - QuickSort","uri":"/quicksort/"}]